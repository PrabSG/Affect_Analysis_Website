<!DOCTYPE html>

<html lang="en">

	<head>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

		<title> Affect Analysis </title>

		<!-- Bootstrap 3 (better than 4) and Fontawesome imports -->

		<link rel="stylesheet" type="text/css" href="css/bootstrap.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css"
			integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">

		<!-- Other style imports (including self-created) -->

		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700,800,600,300' rel='stylesheet' type='text/css'>

	</head>

	<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

		<!-- Loading Screen - has a javascript -->

		<div id="loadingscreen">
			<div id="loadingstatus"> <img src="img/loadingstatus.gif" height="64" width="64" alt=""> </div>
		</div>

		<!-- Navigation Bar - has a javascript for appearing only when landing page is left -->

		<nav class="navbar navbar-custom navbar-fixed-top">
			 <div class="container">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse"> <i class="fa fa-bars"></i> </button>
					<a class="navbar-brand page-scroll" href="#page-top"> <i class="fas fa-podcast"> </i>Affect Analysis</a>
				</div>

				<div class="collapse navbar-collapse navbar-right navbar-main-collapse">
					<ul class="nav navbar-nav">
						<li class="hidden"> <a href="#page-top"></a> </li>
						<li> <a class="page-scroll" href="#introduction" >Basics</a> </li>
						<li> <a class="page-scroll" href="#benefits">Applications</a> </li>
						<li> <a class="page-scroll" href="#databases">Databases</a> </li>
						<li> <a class="page-scroll" href="#howitworks">How It Works</a> </li>
						<li> <a class="page-scroll" href="#more">More</a> </li>
						<li> <a class="page-scroll" href="#refs">Refferences</a> </li>
					</ul>
				</div>
			</div>
        </nav>

		<!-- Landing Page: -->

		<section id="header">
			<div class="header-body">
				<div class="split left">
					<h1>Dimitrios Kollias</h1>
					<a href="#challenges" class ="page-scroll button">Go To Publications Section</a>
				</div>
				<div class="split right">
					<h1>Emotion Recognition & Generation</h1>
					<div class="arrow">
						<span> </span>
						<span> </span>
					</div>
				</div>
			</div>
		</section>

		<!-- The Introduction: -->

		<section id="introduction">
			<div class="container">
				<div class="section-title text-center center">
					<h2>Basics 101</h2>
					<hr>
				</div>
				 <div class="row">
					<div class="col-md-6">
						<div class="introduction-text">
							<h4>What is the Affect Theory?</h4>
								<p>Affect Theory (also known as Affect Analysis) is a theory that seeks to organise 'affects', used interchangeably with 'emotions' into categories, based on different criteria.
								The complexity of this subject makes it an ongoing study in numerous fields, ranging from computer vision to psychology, psychoanalysis and neuroscience.</p>
						</div>
					</div>
					<div class="col-md-6">
						<div class="introduction-text">
							<h4>Core parts of Affect Analysis:</h4>
								<p>There are three core parts that are described on this page:</p>
								<ul>
									<li> <i class="fas fa-plus-circle"> </i>Emotion Generation - essentially, it focuses on modifying facial aspects to simulate different affects.</li>
									<li> <i class="fas fa-eye"> </i>Emotion Recognition - focuses on identyfying different emotions, through different criteria.</li>
									<li> <i class="fas fa-box-open"> </i>Databases - stores different emotions (in the form of videos and images) used to create, train and test different networks for affect recognition and generation.</li>
								</ul>
						</div>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>Classifying Emotions</h2>
					<hr>
				</div>
				
				<div class="row">	
					<div class="text">
						<p>In the research area of emotion recognition, there are several methods of classifying the data obtained through analysing different images that depict persons showing emotions.</p>
					</div>


				<div class="row">
					<p>In the research area of emotion recognition, there are several methods of classifying the data obtained through analysing different images that depict persons showing emotions.</p>

				</div>

				<div class="row">
					<div class="col-md-6">

						<h4>The Seven Basic Emotions:</h4>
						<p>One of the most commonly used framework is classifying the respective image or set of frames according to the 7 basic affects. These are: happiness, anger, fear, disgust, sadness, surprise and the neutral state. However, classifying emotions is sometimes a difficult task, since a very particular emotion could not be as simple in order to be classified using only these 7 ‘buckets'.
					
						<p style="color: #8cf966">Advantages:</p>
						<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Simple - this enables a fast classification of many images in a short time.</li>
						<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Low Numbers - this enables neural networks to better sort images, making training more effective and increasing chances of succes.</li>
							
						<p style="color: #fb4141">Disadvantages:</p>
							
						<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Not Very Representative of Real Life - There are many cases in which humans don't feel one emotion - you can be surprised and happy at the same time.</li>
						<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Simple - Human emotions vary in intesity, and this method doesn't account for this aspect</li>
						<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Surprise and Fear can often be confused with one another, different annotators having different opinions about what different emotions from different individuals represent.</li>

					<div class="introduction-text">
							<h4>The Seven Basic Emotions:</h4>
							<p>One of the most commonly used framework is classifying the respective image or set of frames according to the 7 basic affects. These are: happiness, anger, fear, disgust, sadness, surprise and the neutral state. However, classifying emotions is sometimes a difficult task, since a very particular emotion could not be as simple in order to be classified using only these 7 ‘buckets'.

							<p style="color: #8cf966">Advantages:</p>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Simple - this enables a fast classification of many images in a short time.</li>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Low Numbers - this enables neural networks to better sort images, making training more effective and increasing chances of succes.</li>

							<p style="color: #fb4141">Disadvantages:</p>

							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Not Very Representative of Real Life - There are many cases in which humans don't feel one emotion - you can be surprised and happy at the same time.</li>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Simple - Human emotions vary in intesity, and this method doesn't account for this aspect</li>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Surprise and Fear can often be confused with one another, different annotators having different opinions about what the perceived emotion is.</li>
						</div>

					</div>
				</div>

			</div>
		</section>

		<!-- The Classic: Benefits: -->

		<section id="benefits">
			<div class="container">

				<div class="section-title text-center center">
					<h2>Why Affect Theory?</h2>
					<hr>
				</div>
				
				<div class ="row">
					<div class="text">
						<p>Emotional recognition and generation are important areas of research: a system capable of reliably identifying the emotional state of an individual would open the door to many different technological advancements.</p>
						<p>Example applications of this research:</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<p>Emotion is a fundamental element to human interaction, therefore developing technology that can recognise transitions between different emotional states is essential for developing android bots capable of integrating into a home.  And there is a variety of uses for such robots, from being extremely effective at helping manage emotions by eliminating human biases to actually being able to provide constant care, fulfilling the roles of human caretakers.</p>
					</div>
					<div class="col-md-6 text-center">
						<i class="fas fa-camera-retro"></i>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<i class="fas fa-robot"></i>
					</div>
					<div class="col-md-6">
						<p>Currently, CCTV cameras are used to catch criminals after a crime has already taken place, which is a reactive form of security. With the development of emotion recognition technology, a preventative strategy could be implemented in conjunction with the reactive strategy, in order to improve security. For instance, a system could observe a crowd of people in a public environment and classify each member’s emotional state. Individuals who display suspicious emotions could be questioned by police before they engage in criminal activity.</p>
					</div>
				</div>

			</div>
		</section>

		<!-- Some Database statistics: -->

		<section id="databases">
			<div class="container">

				<div class="section-title text-center center">
					<h2>Emotion Recognition Databases</h2>
					<hr>
				</div>

				<div class="row">
					 <div class="col-md-4">
						<div class="databases-text">
							<h4>A New Challenge...</h4>
								<p>For the past several years, the research area of emotion detection and analysis has faced the need of analysing images showing natural behaviours, rather than controlled ones. Thus, a new challenge was improving previous databases (like ’MMI database’ or ‘Multi-Pie database’) which consist of images (or frames) displaying posed behaviour only. The new approach was trying to capture subjects in an “in the wild" environment,
                                so that the accuracy of the analysis could increase and be more representative for real life situations. However, even such existing datasets have proved to be limited, in the sense that they did not provide enough consistency in order to properly analyse the different representations of emotions. We present here 3 databses which reflect these limitations, having made a comparison table: RECOLA, AFEW and AFEW-VA, and then introduce the new dataset Aff-Wild, tailored to the needs of capturing more genuine expressions.</p>
						</div>
					</div>

					<div class="col-md-4">
						<div class="databases-text">
							<h4>Overcoming Limitations</h4>
							<p>The limitations of existing databases like RECOLA, AFEW or AFEW-VA were mainly due to controlled laboratory conditions (in case of RECOLA only), reduced number of captured frames and very few annotators. A 'controlled laboratory condition' is a situation in which the subject does not show certain emotions in a natural mannner, but is requested to more likely 'imitate' a certain feeling, e.g. happiness. This simulated emotion is then 'annotated' by a specialised person,called an 'annotator', who classifies given images and videos by different criteria on the basis of the 7 basic emotion categories. Moreover, they can also give values in terms of valence and arousal, the annotated information being then used for training. In case of the RECOLA dataset, the subjects were recorded for a total of 9.5 hours and then the annotation process was carried by 6 people. The main downside (apart from the controlled environment) is represented by the very small number of videos. In contrast,
                            the AFEW dataset, which consists of real-world situations, has over 1800 analysed videos, but its drawbacks are the annotation restricted to the basic seven expressions and the total number of frames which is approximately 3 times smaller than the one of RECOLA. Finally, AFEW-VA is limited through the very reduced number of frames (more than 10 times smaller than RACOLA’s) and the discrete values of valance and arousal.</p>
						</div>
					</div>

					<div class="col-md-4">
						<div class="intro-text">
							<h4>The Aff-Wild</h4>
							<p>All these issues with the existing databases lead to the development of the Aff-Wild, a dataset which consists of more than 30 hours of recordings from 298 videos. The main goal of the new dataset was to gather frames reflecting spontaneous behaviour of the subjects. This was captured from a series of YouTube videos in which people react to a variety of situations,
							for instance an unexpected scenario in a film or eating something that they have never eaten before. An important feature of the Aff-Wild is represented by the different cultural backgrounds of the subjects as well as the diverse illumination and occlusion conditions. Another improvement brought by the Aff-Wild is the [-1, 1] interval in which the valence and arousal can range in a continuous manner, the annotation process being carried by 8 annotators. As an overview, the Aff-Wild database provides a very powerful methodology for analysing not only static emotional states of the subjects, but also fast transitions of the facial expressions. </p>
						</div>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>Databases Comparison</h2>
					<hr>
				</div>

				<div class="row text-center">
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-database"> </i>
							<h4>DATABASES:</h4>
							<h3>RECOLA</h3>
							<h3>AFEW</h3>
							<h3>AFEW-VA</h3>
							<h3>Aff-Wild</h3>
						</div>

					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-film"></i>
							<h4>NO. OF FRAMES:</h4>
							<span class="count">345000</span>
							<br><br>
							<span class="count">113355</span>
							<br><br>
							<span class="count">30050</span>
							<br><br>
							<span class="count">1180000</span>
						</div>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-video"> </i>
							<h4>NO. OF VIDEOS:</h4>
							<span class="count">46</span>
							<br><br>
							<span class="count">1809</span>
							<br><br>
							<span class="count">600</span>
							<br><br>
							<span class="count">298</span>
						</div>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-file-signature"> </i>
							<h4>ANNOTATORS:</h4>
							<span class="count">6</span>
							<br><br>
							<span class="count">3</span>
							<br><br>
							<span class="count">2</span>
							<br><br>
							<span class="count">8</span>
						</div>
					</div>
				</div>
			</div>
		</section>

		<!-- How it works Section: -->

		<section id="howitworks">
			<div class="container">
				<div class="section-title text-center center">
					<h2>How Image Recognition and Generation Works</h2>
					<hr>
				</div>


				<div class="row">
					<div class="col-md-6 text-center">
						<br><br><br>
						<i class="fas fa-desktop"></i>
					</div>
					<div class="col-md-6">
						<h4>"Older" Methods - Computer Vision</h4>
						<p>Before the invention of neural networks, computer vision was used for recognising emotions from pictures. Different parts of the face would be evaluated, and depending on the general form it took, it would be categorised in one of the 7 basic emotions (both eyebrows being lifted and the mouth wide opened would most likely mean that the person is scared or surprised). This was evaluated by measuring different body parts and what percentage of the picture they would occupy, or the distances between different elements (a higher distance between eyes and eyebrows means surprise). However, using computer vision is quite limited, as for example a real human can feel several emotions at the same time (you can be happy and surprised at the same time) - it could also not measure the intensity of the affects well.</p>
					</div>
				</div>

				<br><br>

				<div class="row">
					<div class="col-md-6">
						<h4>"Younger" Methods - Neural Networks</h4>
						<p>The recent popularity in neural networks also expanded in this field, due to their high versatility. Through training, they become able to recognise a varied range of emotions, also becoming able to mix different affects between them. Their appearance also made the generation of emotions from neutral states feasible. Two types of neural networks are used, namely CNNs (Convolutional Neural Networks) and RNNs (Recurrent Neural Networks).</p>
					</div>
					<div class="col-md-6 text-center">
						<br><br><br>
						<i class="fas fa-network-wired"></i>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>The CNN and RNN Architectures</h2>
					<h2>Image Recognition</h2>
					<hr>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<br><br><br>

					</div>
					<div class="col-md-6">
						<h4>CNNs - Convolutional Neural Networks</h4>
						<p>Convolution neural network (CNNs) is a class of deep neural networks comprised of two main stages: feature extraction and image classification. CNNs are commonly used in the domain of image processing and can be applied to the specific task of emotional recognition.
						   CNNs utilise supervised learning techniques, therefore, a labeled data set is required to train them to perform a specific task. For the task of identifying emotions, a CNN could be trained using a dataset containing many different images of human faces, which have been classified into (one of 7 states or valence arousal) by psychologists. The accuracy and reliability of the trained neural network is heavily dependent upon the quality of the database used in training. </p>
					</div>
				</div>

				<div class="row">
					<div class="text">
						<p>Essentially, as the name implies, CNNs use convolutions to fulfill their tasks. Those can be understood as “stages” of processing an image. We first start from a 2D image that our brains can easily understand. Then the neural networks starts transposing different parts of the image in higher dimensions (that oftentimes reach great values, even as far as 50D) that are known as hyperspace. This is called spatial analysis, as certain futures are evaluated at different “spaces” (dimensions).
						Each convolution, depending on the results from the previous one, updates itself. After reaching a certain number of convolutions, the networks simply gives the classification it made. A result from a convolution is obtained by attaching a different weight to different features. The weight is then updated by further convolutions (high values means that analysed feature is pronounced and is part of the image). Furthermore, the black box property means that we can’t specify or completely comprehend the “features” that the CNN will pick out.
						As it is trained, it will learn its own patterns and signals that (as a machine) it would notice which may not necessarily be the same markers that we as humans use for emotion recognition.</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<h4>RNNs - Recurrent Neural Networks</h4>
						<p>Recurrent Neural Networks take in an input state as well as an input to process, functioning similar to a state machine. Their main feature is that it can use the same node to process all of the data, however even given the same input the node or ‘cell’ could produce a different output. This is because the output also depends on the previous state that is fed into the cell. As a result RNNs are effective at pattern replication and sequence prediction: given an input they can produce an output depending on previous patterns and sequences.
						   This is useful when dealing with data that comes in a specified sequence such as text or video. The cells themselves apply some mathematical functions to the input, dependent on the input state, and then produce both an output of the network and another state that can be fed back into itself for the next input.
						   Alternatively, the cells can be “rolled out” so that there are several cells in sequence so that certain cells could apply functions that are slightly different, or have differently adjusted hyperparameters in their functions.
						   For example, if a sentence was passed into a sequence of RNN cells, you might expect that some of the middle cells would be more likely to output verbs, and the opposite for the last ones as its unlikely to end a sentence in a verb.</p>
					</div>
					<div class="col-md-6 text-center">
						<br><br><br><br><br>
						<img src="img/rnns.png" class="img-responsive">
					</div>
				</div>

				<div class="row">
					<div class="text">
						<p>Furthemore, these cells can be stacked ontop of eachother allowing for a second round of data processing to be done that may be looking at a different aspect of the data. This would also carry its own input state so that there would be several input/output states used.
						This approach somewhat mimics the parallel processing approach used in CNNs when they utilise several layers to detect a different part of input data.</p>
						<p>Within the realm of emotion recognition, both CNNs and RNNs are used for different purposes, but together to solve the ultimate goal of recognising and then assigning a specific value to facial expressions.
						We can also process not only images but whole videos. This is where the RNNs are used. The RNNs are specifically made for and used for handling temporal data, or when there needs to be some sort of short-term memory to process data and interpret it by also remembering what came before it.
						In a video (essentially a sequence of images) we can pick out some of the key frames of the video and . We don’t utilise every frame as there is a miniscule difference between every pair of neighbouring frames and processing all of them would result in wasted computational power and time. Then when the RNN calculates its output of what emotion it believes is being represented by the video, it utilises the sequence of frames before hand to see how the face was changing.
						For example, if the first processed frame was a neutral face, then the eyes started the widen, then in the next frame the mouth began to open, then the cheeks began to raise and tense, this sequence of actions could indicate that the person is happily surprised or excited. However, from those individual frames it may be much harder to realise that is the emotion that is being expressed. So in combination, the CNN works on the feature extraction of each frame, and the RNN interprets how that sequence of frames is connected to recognise an emotion.</p>
					</div>
				</div>

				<div class="section-title text-center center">
					<h2>Image Generation</h2>
					<hr>
				</div>

				<div class="row">

				</div>

			</div>
		</section>



		<!-- The footer -->

		<section id="footer">
			<div class="container">
				<div class="row">
					<div class="col-md-3 col-sm-6">
						<div class="row">

							<div class="cod-md-5 col-sm-7">
								<i class="far fa-user" data-toggle="modal" data-target="#modalCalin"> </i>
								<p>Calin</p>
							</div>

							<div class="cod-md-4 col-sm-3">
								<i class="far fa-user" data-toggle="modal" data-target="#modalAndrei"> </i>
								<p>Andrei</p>
							</div>
						</div>
					</div>
					<div class="col-md-6 col-sm-6">
						<h3>Designed by Andrei Roman, Calin Biberea, Prabhjot Grewal, Daniel Kaye</h3>
						<p>Click the icons to see some impressions of this topic from them (and just some contacts)</p>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="row">
							<div class="cod-md-5 col-sm-7">
								<i class="far fa-user" data-toggle="modal" data-target="#modalPrabhjot"> </i>
								<p>Prabhjot</p>
							</div>
							<div class="cod-md-4 col-sm-3">
								<i class="far fa-user" data-toggle="modal" data-target="#modalDaniel"> </i>
								<p>Daniel</p>
							</div>
						</div>
					</div>
				</div>
			</div>

			<div id="modalCalin" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Calin's Impressions</h4>
						</div>
						<div class="modal-body">
							<h1>TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT</h1>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>cb3418@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalAndrei" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Andrei Roman</h4>
						</div>
						<div class="modal-body">
						<img src="img/andrei.jpg" align="left" width="70px" height="80px">
						<h4> Undergraduate Student - Department of Computing  <h4 style="color:blue;">Imperial College London</h4> </h4>
				    <h4> Course: BEng Computing</h4>
						</div>
						<div class="modal-footer">
							<p>Want to contact Andrei? Mail him at <a>amr818@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalPrabhjot" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Prabhjot's Impressions</h4>
						</div>
						<div class="modal-body">
							<h1>TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT</h1>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>tsg918@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalDaniel" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Daniel's Impressions</h4>
						</div>
						<div class="modal-body">
							<h1>TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT</h1>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>djk18@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>


		</section>

		<!-- JQuery import: -->

		<script src="js/JQuery.js"></script>

		<!-- Other javascripts that are imported: -->

		<script src="js/bootstrap.js"></script>
		<script src="js/SmoothScroll.js"></script>
		<script src="js/jquery.isotope.js"></script>
		<script src="js/jquery.parallax.js"></script>

		<!-- Javascripts that we created: -->

		<script src="js/main.js"> </script>
		<script src="js/header.js"> </script>
		<script src="js/counter.js"> </script>

	</body>

</html>
