<!DOCTYPE html>

<html lang="en">

	<head>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

		<title> Affect Analysis </title>

		<!-- Bootstrap 3 (better than 4) and Fontawesome imports -->

		<link rel="stylesheet" type="text/css" href="css/bootstrap.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css"
			integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">

		<!-- Other style imports (including self-created) -->

		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700,800,600,300' rel='stylesheet' type='text/css'>

	</head>

	<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

		<!-- Loading Screen - has a javascript -->

		<div id="loadingscreen">
			<div id="loadingstatus"> <img src="img/loadingstatus.gif" height="64" width="64" alt="Loading GIF"> </div>
		</div>

		<!-- Navigation Bar - has a javascript for appearing only when landing page is left -->

		<nav class="navbar navbar-custom navbar-fixed-top">
			 <div class="container">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse"> <i class="fa fa-bars"></i> </button>
					<a class="navbar-brand page-scroll" href="#page-top"> <i class="fas fa-podcast"> </i>Affect Analysis</a>
				</div>

				<div class="collapse navbar-collapse navbar-right navbar-main-collapse">
					<ul class="nav navbar-nav">
						<li class="hidden"> <a href="#page-top"></a> </li>
						<li> <a class="page-scroll" href="#introduction">Basics</a> </li>
						<li> <a class="page-scroll" href="#benefits">Applications</a> </li>
						<li> <a class="page-scroll" href="#databases">Databases</a> </li>
						<li> <a class="page-scroll" href="#howitworks">How It Works</a> </li>
						<li> <a class="page-scroll" href="#about">About</a> </li>
						<li> <a class="page-scroll" href="#refs">Refferences</a> </li>
					</ul>
				</div>
			</div>
        </nav>

		<!-- Landing Page: -->

		<section id="header">
			<div class="header-body">
				<div class="split left">
					<h1>Dimitrios Kollias</h1>
					<a href="#about" class ="page-scroll button">Go To About Section</a>
				</div>
				<div class="split right">
					<h1>Emotion Recognition & Generation</h1>
					
					
					
					<a class="page-scroll" href="#introduction">
						<div class="arrow page-scroll">
							<span> </span>
							<span> </span>
						</div>
					</a>
				</div>
			</div>
		</section>

		<!-- The Introduction: -->

		<section id="introduction">
			<div class="container">
				<div class="section-title text-center center">
					<h2>Basics 101</h2>
					<hr>
				</div>
				 <div class="row">
					<div class="col-md-6">
						<div class="introduction-text">
							<h4>What is the Affect Theory?</h4>
								<p>Affect Theory (also known as Affect Analysis) is a theory that seeks to organise 'affects' (used interchangeably with 'emotions') into categories, based on different criteria.
								The complexity of this subject makes it an ongoing study in numerous fields, ranging from computer vision to psychology, psychoanalysis and neuroscience.</p>
						</div>
					</div>
					<div class="col-md-6">
						<div class="introduction-text">
							<h4>Core parts of Affect Analysis:</h4>
								<p>There are three core parts that are described on this page:</p>
								<ul>
									<li> <i class="fas fa-plus-circle"> </i>Emotion Generation - essentially, it focuses on modifying facial aspects to simulate different affects.</li>
									<li> <i class="fas fa-eye"> </i>Emotion Recognition - focuses on identyfying different emotions, through different criteria.</li>
									<li> <i class="fas fa-box-open"> </i>Databases - stores different emotions (in the form of videos and images) used to create, train and test different networks for affect recognition and generation.</li>
								</ul>
						</div>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>Classifying Emotions</h2>
					<hr>
				</div>
				
				<div class="row">	
					<div class="text">
						<p>In the research area of emotion recognition, there are several methods of classifying the data obtained through analysing different images that depict persons showing emotions.</p>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-6">

						<h4>The Seven Basic Emotions:</h4>
						<p>One of the most commonly used frameworks is classifying the respective image or set of frames according to the 7 basic affects. These are: happiness, anger, fear, disgust, sadness, surprise and the neutral state. However, classifying emotions is sometimes a difficult task, since a very particular emotion could not be as simple in order to be classified using only these 7 ‘buckets'.
					
						<p style="color: #8cf966">Advantages:</p>
						<ul>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Clarification - this enables a fast classification of many images in a short time.</li>
						</ul>
						
						<p style="color: #fb4141">Disadvantages:</p>
						<ul>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Not Very Representative of Real Life - there are many cases in which humans don't feel one emotion - you can be surprised and happy at the same time.</li>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Simple - human emotions vary in intesity, and this method doesn't account for this aspect</li>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Surprise and Fear can often be confused with one another, different annotators having various interpretations about what emotion a specific facial expression represents.</li>
						</ul>
						
					</div>
					
					<!-- Gallery Elements: -->
					
					<div class="col-md-6">
						<div id="slideshow">
						
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/rose-red-wine.jpg" alt="rose-red-wine">
							<figcaption>Rose Red Wine</figcaption>
						</figure>
					
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/green-glass-bottle.jpg" alt>
							<figcaption>Green Glass Bottle</figcaption>
						</figure>
					
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/guinness-barrels.jpg" alt>
							<figcaption>Guinness Barrels, Dublin</figcaption>
						</figure>
					
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/crystal-skull-vodka.jpg" alt>
							<figcaption>Crystal Skull Vodka</figcaption>
						</figure>
						</div>

						<div id="slideshow">
						
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/crystal-skull-vodka.jpg" alt>
							<figcaption>Crystal Skull Vodka</figcaption>
						</figure>
						
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/green-glass-bottle.jpg" alt>
							<figcaption>Green Glass Bottle</figcaption>
						</figure>
						
						<figure>
							<img src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/4273/rose-red-wine.jpg" alt="rose-red-wine">
							<figcaption>Rose Red Wine</figcaption>
						</figure>
						</div>
					
					</div>
					
					<div class="col-md-6 text-center">
						<p>The Seven Basic Emotions</p>
					</div>
					
				</div>
				
				<br>
				
				<div class="row">
					<div class="text">
						<h4>Compound Emotions:</h4>
						<p>An evolution from the '7 basic emotions' that has become more and more used in the last years is 'compound emotions'. This essentially unites two emotions into a set, such as happy-surprised. Although it is an improvement, the drawbacks of the 'seven basic emotions' system are found here as well.</p>
					</div>
				</div>
				
				<div class="row">
					<div class="text">
						<h4>Action Units:</h4>
						<p>A way of categorizing and recognising emotions is through measuring the muscle movement that they determine, a parameter called 'action units'. Each muscular fiber is analysed (e.g.the elevation of one eyebrow and the descent of the other one), also taking into account the amount of movement of each muscle, thus evaluating intensity. The rules used for this classification are deterministic: that is,the "activation" of some muscles and the lack of contraction in others can indicate a specific and definite emotion.</p>
					</div>
				</div>
				
				<div class="row">
					<div class="col-md-6 text-center">
						<br>
						<img src="img/valencearousal.png" class="img-responsive" alt="The Valence Arousal 2D Wheel">
						<p>The Emotion 2D Wheel</p>
						<p>Source: Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond</p>
					</div>
					
					<div class="col-md-6">
						<h4>The Valence-Arousal Space</h4>
						<p>This is another way of classifying emotions, that uses a regressive approach in their measurement. This means that emotions are attributed a value, which can be floating, (like being 1.5 happy), making emotion classification even more realistic. Valence evaluates whether(and to what extent) an emotion is positive or negative, while arousal evaluates if the emotion is active.
						
						<p style="color: #8cf966">Advantages:</p>
						<ul>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Complex - this enables classifying more general feelings, that comprise multiple emotions.</li>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Realistic - it is the best representation that we have for affects, as it is allows us to accurately classify natural behaviour.</li>
						</ul>
						
						<p style="color: #fb4141">Disadvantages:</p>
						<ul>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Subjective - because now affects are given continuous values, different annotators cand wield dfferent results (one sees an intesity of 0.6 while the other sees an intensity of 0.3).</li>						
						</ul>
					</div>
				</div>
			</div>
		</section>

		<!-- The Classic: Benefits: -->

		<section id="benefits">
			<div class="container">

				<div class="section-title text-center center">
					<h2>Why 'Affect Theory'?</h2>
					<hr>
				</div>
				
				<div class ="row">
					<div class="text">
						<p>Emotion recognition and generation are important areas of research: a system capable of reliably identifying the emotional state of an individual would open the door to many different technological advancements.</p>
						<p>Applications of this research:</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<p>Emotion is a fundamental element of human interaction, therefore developing technology that can recognise transitions between different emotional states is essential for developing android bots that can be integrated in a society.  There is a variety of uses for such robots: from being extremely effective in intervening in case of emotional distress, by eliminating human biases, to actually being able to provide constant care, fulfilling the roles of human caretakers.</p>
					</div>
					<div class="col-md-6 text-center">
						<i class="fas fa-camera-retro"></i>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<i class="fas fa-robot"></i>
					</div>
					<div class="col-md-6">
						<p>Currently, CCTV cameras are used to catch criminals after a crime has already taken place, which is a reactive form of security. With the development of emotion recognition technology, a preventative strategy could be implemented in conjunction with the reactive strategy, in order to improve security. For instance, a system could assess a crowd of people in a public environment and classify each member’s emotional state. Individuals who display suspicious emotions could be questioned by police before they engage in criminal activity.</p>
					</div>
				</div>

			</div>
		</section>

		<!-- Some Database statistics: -->

		<section id="databases">
			<div class="container">

				<div class="section-title text-center center">
					<h2>Emotion Recognition Databases</h2>
					<hr>
				</div>

				<div class="row">
					 <div class="col-md-6">
						<h4>A New Challenge...</h4>
						<p>In recent years, the research area of emotion detection and analysis has faced the need of analysing images showing natural behaviours, rather than controlled ones. Thus, a new challenge that appeared was improving previous databases (like ’MMI database’ or ‘Multi-Pie database’) which consist of images (or frames) displaying posed behaviour only(acted emotions on comands). The new approach was trying to capture subjects in an “in the wild" environment,                          so that the accuracy of the analysis could increase and be more representative for real life situations. However, even such existing datasets have proved to be limited, in the sense that they did not provide enough consistency in order to properly analyse the different representations of emotions. We present here 3 databses which reflect these limitations, having made a comparison table: RECOLA, AFEW and AFEW-VA, and then introduce the new dataset Aff-Wild, tailored to the needs of capturing more genuine expressions.</p>
					</div>

					<div class="col-md-6">
						<h4>Overcoming Limitations</h4>
						<p>The limitations of existing databases like RECOLA, AFEW or AFEW-VA were mainly caused by controlled laboratory conditions (solely in case of RECOLA), reduced number of captured frames and very few annotators. A 'controlled laboratory condition' is a situation in which the subject does not show certain emotions in a natural mannner, but is requested to more likely 'imitate' a certain feeling (e.g. happiness). This simulated emotion is then 'annotated' by a specialised person,called an 'annotator', who classifies given images and videos by different criteria on the basis of the '7 basic emotions' categories. Moreover, they can also assign values of valence and arousal, the annotated information being then used for training. In case of the RECOLA dataset, the subjects were recorded for a total of 9.5 hours and then the annotation process was carried out by 6 people. The main downside of this (apart from the controlled environment) is the very small number of videos. In contrast,
                        the AFEW dataset, which consists of real-world situations, has over 1800 analysed videos, but its drawbacks are that the annotation is restricted to the basic seven expressions and that the total number of frames, which is approximately 3 times smaller than the one of RECOLA. Finally, AFEW-VA is limited by the very reduced number of frames (more than 10 times smaller than RACOLA’s) and the discrete values of valance and arousal.</p>
					</div>

				</div>
				
				<div class="row">
					<div class="text">		
						<h4>The Aff-Wild</h4>
						<p>All these issues in the existing databases led to the development of the Aff-Wild, a dataset which consists of more than 30 hours of recordings from 298 videos. The main goal of the new dataset was to gather frames reflecting spontaneous behaviour of the subjects. They were captured from a series of YouTube videos, in which people react to a variety of situations,
						for instance an unexpected scenario in a film, or eating something that they have never eaten before. An important feature of the Aff-Wild is represented by the different cultural backgrounds of the subjects as well as the diverse illumination and occlusion conditions. Another improvement brought by the Aff-Wild is the [-1, 1] interval, in which the valence and arousal can range in a continuous manner, the annotation process being carried out by 8 annotators. As an overview, the Aff-Wild database provides a very powerful methodology for analysing not only static emotional states of the subjects, but also fast transitions of the facial expressions. </p>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>Databases Comparison</h2>
					<hr>
				</div>

				<div class="row text-center">
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-database"> </i>
							<h4>DATABASES:</h4>
							<h3>RECOLA</h3>
							<h3>AFEW</h3>
							<h3>AFEW-VA</h3>
							<h3>Aff-Wild</h3>
						</div>

					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-film"></i>
							<h4>NO. OF FRAMES:</h4>
							<span class="count">345000</span>
							<br><br>
							<span class="count">113355</span>
							<br><br>
							<span class="count">30050</span>
							<br><br>
							<span class="count">1180000</span>
						</div>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-video"> </i>
							<h4>NO. OF VIDEOS:</h4>
							<span class="count">46</span>
							<br><br>
							<span class="count">1809</span>
							<br><br>
							<span class="count">600</span>
							<br><br>
							<span class="count">298</span>
						</div>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-file-signature"> </i>
							<h4>ANNOTATORS:</h4>
							<span class="count">6</span>
							<br><br>
							<span class="count">3</span>
							<br><br>
							<span class="count">2</span>
							<br><br>
							<span class="count">8</span>
						</div>
					</div>
				</div>
			</div>
		</section>

		<!-- How it works Section: -->

		<section id="howitworks">
			<div class="container">
				<div class="section-title text-center center">
					<h2>How Image Recognition and Generation Works</h2>
					<hr>
				</div>


				<div class="row">
					<div class="col-md-6 text-center">
						<br><br><br>
						<i class="fas fa-desktop"></i>
					</div>
					<div class="col-md-6">
						<h4>"Older" Methods - Computer Vision</h4>
						<p>Computer Vision proved in time to be a multi purpose tool for a variety of uses. One of those uses today is recognising and generating affects.</p>
						<p>Different parts of the face would be evaluated, and depending on the patterns of muscle contractions, they would be categorised in one of the 7 basic emotions (both eyebrows lifted and mouth wide opened most likely means that the person is scared or surprised). This was evaluated by measuring different body parts and what percentage of the picture they occupy, or the distance between the main components of the face (a higher distance between eyes and eyebrows means surprise).</p>
					</div>
				</div>

				<br><br>

				<div class="row">
					<div class="col-md-6">
						<h4>"Younger" Methods - Neural Networks</h4>
						<p>The recent popularity of neural networks also expanded in this field, due to their high versatility. Through training, they become able to recognise a various range of emotions, also becoming able to mix different affects. Two types of neural networks are used, namely CNNs (Convolutional Neural Networks) and RNNs (Recurrent Neural Networks).</p>
					</div>
					<div class="col-md-6 text-center">
						<br><br><br>
						<i class="fas fa-network-wired"></i>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>The CNN and RNN Architectures</h2>
					<h2>Image Recognition</h2>
					<hr>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<br><br><br>
						<img src="img/cnn.png" class="img-responsive" alt="A CNN Example">
						<p>A Basic CNN Example</p>
						<p>Source: Wikipedia, <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks</a></p>
					</div>
					<div class="col-md-6">
						<h4>CNNs - Convolutional Neural Networks</h4>
						<p>Convolution neural network (CNNs) is a class of deep neural networks comprised of two main stages: feature extraction and image classification. CNNs are commonly used in the domain of image processing and can be applied to the specific task of emotional recognition.
						   CNNs utilise supervised learning techniques, therefore, a labeled data set is required to train them to perform a specific task. For the task of identifying emotions, a CNN could be trained using a dataset containing many different images of human faces, which have been classified into one of 7 states or valence arousal by psychologists. The accuracy and reliability of the trained neural network is heavily dependent upon the quality of the database used in training. </p>
					</div>
				</div>

				<div class="row">
					<div class="text">
						<p>Essentially, as the name implies, CNNs use convolutions to fulfill their tasks. Those can be understood as “stages” of processing an image. We first start from a 2D image that our brains can easily understand. Then the neural networks starts transposing different parts of the image in higher dimensions (that oftentimes reach great values, even as far as 50D) that are known as hyperspace. This is called spatial analysis, as certain futures are evaluated at different “spaces” (dimensions).
						Each convolution, depending on the results from the previous one, updates itself. After reaching a certain number of convolutions, the networks simply gives the classification it made. A result from a convolution is obtained by attaching a different weight to different features. The weight is then updated by further convolutions (high values means that analysed feature is pronounced and is part of the image). Furthermore, the black box property means that we can’t specify or completely comprehend the “features” that the CNN will pick out.
						As it is trained, it will learn its own patterns and signals that (as a machine) it would notice which may not necessarily be the same markers that we as humans use for emotion recognition.</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<h4>RNNs - Recurrent Neural Networks</h4>
						<p>Recurrent Neural Networks take in an input state as well as an input to process, functioning similar to a state machine. Their main feature is that it can use the same node to process all of the data. However, even given the same input the node or ‘cell’ could produce a different output. This is because the output also depends on the previous state that is fed into the cell. As a result RNNs are effective at pattern replication and sequence prediction: given an input, they can produce an output, depending on previous patterns and sequences.
						   This is useful when dealing with data that comes in a specified sequence, such as text or video. The cells themselves apply some mathematical functions to the input, dependent on the input state, and then produce both an output of the network and another state, that can be fed back into itself for the next input.
						   Alternatively, the cells can be “rolled out” so that there are several cells in sequence, so that certain cells could apply functions that are slightly different, or have differently adjusted hyperparameters in their functions.
						   For example, if a sentence was passed into a sequence of RNN cells, you might expect that some of the middle cells would be more likely to output verbs, and the opposite for the last ones, as it's unlikely to end a sentence in a verb.</p>
					</div>
					<div class="col-md-6 text-center">
						<br><br><br><br><br>
						<img src="img/rnns.png" class="img-responsive" alt="A RNN Diagram">
					</div>
				</div>

				<div class="row">
					<div class="text">
						<p>Furthemore, these cells can be stacked on top of each other, allowing for a second round of data processing(that may be looking at a different aspect of the data) to be done. This would also carry its own input state so that there would be several input/output states used.
						This approach somewhat mimics the parallel processing approach used in CNNs when they utilise several layers to detect a different part of input data.</p>
						<p>Within the realm of emotion recognition, both CNNs and RNNs are used for different purposes, but together to solve the ultimate goal of recognising and then assigning a specific value to facial expressions.
						We can also process not only images but whole videos. This is where the RNNs are used. The RNNs are specifically made for and used for handling temporal data, or when there needs to be some sort of short-term memory to process data and interpret it by also remembering what came before it.
						In a video (essentially a sequence of images) we can pick out some of the key frames of the video and . We don’t utilise every frame as there is a miniscule difference between every pair of neighbouring frames and processing all of them would result in wasted computational power and time. Then when the RNN calculates its output of what emotion it believes is being represented by the video, it utilises the sequence of frames before hand to see how the face was changing.
						For example, if the first processed frame was a neutral face, then the eyes started the widen, then in the next frame the mouth began to open, then the cheeks began to raise and tense, this sequence of actions could indicate that the person is happily surprised or excited. However, from those individual frames it may be much harder to realise that is the emotion that is being expressed. So in combination, the CNN works on the feature extraction of each frame, and the RNN interprets how that sequence of frames is connected to recognise an emotion.</p>
					</div>
				</div>

				<div class="section-title text-center center">
					<h2>Image Generation</h2>
					<hr>
				</div>

				<div class="row">

				</div>

			</div>
		</section>

		<!-- About Section -->
		
		<section id="about">
		</section>

		<!-- The footer -->

		<section id="footer">
			<div class="container">
				<div class="row">
					<div class="col-md-3 col-sm-6">
						<div class="row">

							<div class="cod-md-5 col-sm-7">
								<i class="far fa-user" data-toggle="modal" data-target="#modalCalin"> </i>
								<p>Calin</p>
							</div>

							<div class="cod-md-4 col-sm-3">
								<i class="far fa-user" data-toggle="modal" data-target="#modalAndrei"> </i>
								<p>Andrei</p>
							</div>
						</div>
					</div>
					<div class="col-md-6 col-sm-6">
						<h3>Designed by Andrei Roman, Calin Biberea, Prabhjot Grewal, Daniel Kaye</h3>
						<p>Click the icons to see some impressions of this topic from them (and just some contacts)</p>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="row">
							<div class="cod-md-5 col-sm-7">
								<i class="far fa-user" data-toggle="modal" data-target="#modalPrabhjot"> </i>
								<p>Prabhjot</p>
							</div>
							<div class="cod-md-4 col-sm-3">
								<i class="far fa-user" data-toggle="modal" data-target="#modalDaniel"> </i>
								<p>Daniel</p>
							</div>
						</div>
					</div>
				</div>
			</div>

			<div id="modalCalin" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Calin's Impressions</h4>
						</div>
						<div class="modal-body">
							<h1>TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT</h1>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>cb3418@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalAndrei" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Andrei Roman</h4>
						</div>
						<div class="modal-body">
							<h4> Undergraduate Student - Department of Computing</h4>  
							<h4 style="color:blue;">Imperial College London</h4> 
							<h4> Course: BEng Computing</h4>
						</div>
						<div class="modal-footer">
							<p>Want to contact Andrei? Mail him at <a>amr818@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalPrabhjot" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Prabhjot's Impressions</h4>
						</div>
						<div class="modal-body">
							<h1>TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT</h1>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>tsg918@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalDaniel" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Daniel's Impressions</h4>
						</div>
						<div class="modal-body">
							<h1>TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT</h1>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>djk18@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>


		</section>

		<!-- JQuery import: -->

		<script src="js/JQuery.js"></script>

		<!-- Other javascripts that are imported: -->

		<script src="js/bootstrap.js"></script>
		<script src="js/SmoothScroll.js"></script>
		<script src="js/jquery.isotope.js"></script>
		<script src="js/jquery.parallax.js"></script>

		<!-- Javascripts that we created: -->

		<script src="js/main.js"> </script>
		<script src="js/header.js"> </script>
		<script src="js/gallery.js"> </script>
		<script src="js/counter.js"> </script>

	</body>

</html>
