<!DOCTYPE html>

<html lang="en">

	<head>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

		<title> Affect Analysis </title>

		<!-- Bootstrap 3 (better than 4) and Fontawesome imports -->

		<link rel="stylesheet" type="text/css" href="css/bootstrap.css">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css"
			integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">

		<!-- Other style imports (including self-created) -->

		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700,800,600,300' rel='stylesheet' type='text/css'>

	</head>

	<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

		<!-- Loading Screen - has a javascript -->

		<div id="loadingscreen">
			<div id="loadingstatus"> <img src="img/loadingstatus.gif" height="64" width="64" alt="Loading GIF"> </div>
		</div>

		<!-- Navigation Bar - has a javascript for appearing only when landing page is left -->

		<nav class="navbar navbar-custom navbar-fixed-top">
			 <div class="container">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse"> <i class="fa fa-bars"></i> </button>
					<a class="navbar-brand page-scroll" href="#page-top"> <i class="fas fa-podcast"> </i>Affect Analysis</a>
				</div>

				<div class="collapse navbar-collapse navbar-right navbar-main-collapse">
					<ul class="nav navbar-nav">
						<li class="hidden"> <a href="#page-top"></a> </li>
						<li> <a class="page-scroll" href="#introduction">Basics</a> </li>
						<li> <a class="page-scroll" href="#benefits">Applications</a> </li>
						<li> <a class="page-scroll" href="#databases">Databases</a> </li>
						<li> <a class="page-scroll" href="#howitworks">How It Works</a> </li>
						<li> <a class="page-scroll" href="#about">About</a> </li>
						<li> <a class="page-scroll" href="#reference">Refferences</a> </li>
					</ul>
				</div>
			</div>
        </nav>

		<!-- Landing Page: -->

		<section id="header">
			<div class="header-body">
				<div class="split left">
					<h1>Dimitrios Kollias</h1>
					<a href="#about" class ="page-scroll button">Go To About Section</a>
				</div>
				<div class="split right">
					<h1>Emotion Recognition & Generation</h1>
					<div id="particles-js"> </div>
					<a class="page-scroll" href="#introduction">
						<div class="arrow page-scroll">
							<span> </span>
							<span> </span>
						</div>
					</a>
				</div>
			</div>
		</section>

		<!-- The Introduction: -->

		<section id="introduction">
			<div class="container">
				<div class="section-title text-center center">
					<h2>Basics 101</h2>
					<hr>
				</div>
				 <div class="row">
					<div class="col-md-6">
						<div class="introduction-text">
							<h4>What is the Affect Theory?</h4>
								<p>Affect Theory (also known as Affect Analysis) is a theory that seeks to organise 'affects' (used interchangeably with 'emotions') into categories, based on different criteria.
								The complexity of this subject makes it an ongoing study in numerous fields, ranging from computer vision to psychology, psychoanalysis and neuroscience.</p>
						</div>
					</div>
					<div class="col-md-6">
						<div class="introduction-text">
							<h4>Core parts of Affect Analysis:</h4>
								<p>There are three core parts that are described on this page:</p>
								<ul>
									<li> <i class="fas fa-plus-circle"> </i>Emotion Generation - essentially, it focuses on modifying facial aspects to simulate different affects.</li>
									<li> <i class="fas fa-eye"> </i>Emotion Recognition - focuses on identyfying different emotions, through different criteria.</li>
									<li> <i class="fas fa-box-open"> </i>Databases - stores different emotions (in the form of videos and images) used to create, train and test different networks for affect recognition and generation.</li>
								</ul>
						</div>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>Classifying Emotions</h2>
					<hr>
				</div>

				<div class="row">
					<div class="text">
						<p>In the research area of emotion recognition, there are several methods of classifying the data obtained through analysing different images that depict persons showing emotions.</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">

						<h4>The Seven Basic Emotions:</h4>
						<p>One of the most commonly used frameworks is classifying the respective image or set of frames according to the 7 basic affects. These are: happiness, anger, fear, disgust, sadness, surprise and the neutral state. However, classifying emotions is sometimes a difficult task, since a very particular emotion could not be as simple in order to be classified using only these 7 ‘buckets'.

						<p style="color: #8cf966">Advantages:</p>
						<ul>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Simple (only seven categories) - as the values are discrete (distinct from one another), it makes the process of classifying an image easier in some case, as it is easy to eliminate cases (a sad face is not similar to a happy one, therefore the query is faster).</li>
						</ul>

						<p style="color: #fb4141">Disadvantages:</p>
						<ul>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Not Very Representative of Real Life - there are many cases in which humans don't feel one emotion - you can be surprised and happy at the same time.</li>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Simple - human emotions vary in intesity, and this method doesn't account for this aspect</li>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Surprise and Fear can often be confused with one another, different annotators having various interpretations about what emotion a specific facial expression represents.</li>
						</ul>

					</div>

					<!-- Gallery Elements: -->

					<div class="col-md-6">
						<div class="slideshow">

						<figure>
							<img src="img/neutralstate.jpg" alt="Neutral State">
							<figcaption>Neutral State</figcaption>
						</figure>

						<figure>
							<img src="img/scaredstate.jpg" alt="Scared State">
							<figcaption>Scared State</figcaption>
						</figure>

						<figure>
							<img src="img/sadstate.jpg" alt="Sad State">
							<figcaption>Sad State</figcaption>
						</figure>

						<figure>
							<img src="img/prajitstate.jpg" alt="Happy State">
							<figcaption>Happy State</figcaption>
						</figure>
						</div>

						<div class="slideshow">

						<figure>
							<img src="img/disgustedstate.jpg" alt="Disgusted State">
							<figcaption>Disgusted State</figcaption>
						</figure>

						<figure>
							<img src="img/angerystate.jpg" alt="Angry State">
							<figcaption>Angry State</figcaption>
						</figure>

						<figure>
							<img src="img/surprisedstate.jpg" alt="Surprised State">
							<figcaption>Surprised State</figcaption>
						</figure>
						</div>
						
						<div class="text-center">
							<p>The Seven Basic Emotions</p>
							<p>PS: Click one to see the presented state</p>
						</div>

					</div>
				</div>

				<br>

				<div class="row">
					<div class="text">
						<h4>Compound Emotions:</h4>
						<p>An evolution from the '7 basic emotions' that has become more and more used in the last years is 'compound emotions'. This essentially unites two emotions into a set, such as happy-surprised. Although it is an improvement, the drawbacks of the 'seven basic emotions' system are found here as well.</p>
					</div>
				</div>

				<div class="row">
					<div class="text">
						<h4>Action Units:</h4>
						<p>A way of categorizing and recognising emotions is through measuring the muscle movement that they determine, a parameter called 'action units'. Each muscular fiber is analysed (e.g.the elevation of one eyebrow and the descent of the other one), also taking into account the amount of movement of each muscle, thus evaluating intensity. The rules used for this classification are deterministic: that is,the "activation" of some muscles and the lack of contraction in others can indicate a specific and definite emotion.</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<br>
						<img src="img/valencearousal.png" class="img-responsive text-center" alt="The Valence Arousal 2D Wheel" style="padding-left: 30px;">
						<p>The 2D Emotion Wheel</p>
						<p>Source: <a href="https://arxiv.org/pdf/1804.10938.pdf" target=_blank> Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond </a> </p>
					</div>

					<div class="col-md-6">
						<h4>The Valence-Arousal Space</h4>
						<p>This is another way of classifying emotions, that uses a regressive approach in their measurement. This means that emotions are attributed a value, which can be floating, (like being 1.5 happy), making emotion classification even more realistic. Valence evaluates whether(and to what extent) an emotion is positive or negative, while arousal evaluates if the emotion is active.

						<p style="color: #8cf966">Advantages:</p>
						<ul>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Complex - this enables classifying more general feelings, that comprise multiple emotions.</li>
							<li> <i class="fas fa-plus-circle" style="color: #8cf966"> </i>Realistic - it is the best representation that we have for affects, as it is allows us to accurately classify natural behaviour.</li>
						</ul>

						<p style="color: #fb4141">Disadvantages:</p>
						<ul>
							<li> <i class="fas fa-minus-circle" style="color: #fb4141"> </i>Subjective - because now affects are given continuous values, different annotators cand wield dfferent results (one sees an intesity of 0.6 while the other sees an intensity of 0.3).</li>
						</ul>
					</div>
				</div>
			</div>
		</section>

		<!-- The Classic: Benefits: -->

		<section id="benefits">
			<div class="container">

				<div class="section-title text-center center">
					<h2>Why 'Affect Theory'?</h2>
					<hr>
				</div>

				<div class ="row">
					<div class="text">
						<p>Emotion recognition and generation are important areas of research: a system capable of reliably identifying the emotional state of an individual would open the door to many different technological advancements.</p>
						<p>Applications of this research:</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<p>Emotion is a fundamental element of human interaction, therefore developing technology that can recognise transitions between different emotional states is essential for developing android bots that can be integrated in a society.  There is a variety of uses for such robots: from being extremely effective in intervening in case of emotional distress, by eliminating human biases, to actually being able to provide constant care, fulfilling the roles of human caretakers.</p>
					</div>
					<div class="col-md-6 text-center">
						<i class="fas fa-camera-retro"></i>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<i class="fas fa-robot"></i>
					</div>
					<div class="col-md-6">
						<p>Currently, CCTV cameras are used to catch criminals after a crime has already taken place, which is a reactive form of security. With the development of emotion recognition technology, a preventative strategy could be implemented in conjunction with the reactive strategy, in order to improve security. For instance, a system could assess a crowd of people in a public environment and classify each member’s emotional state. Individuals who display suspicious emotions could be questioned by police before they engage in criminal activity.</p>
					</div>
				</div>

			</div>
		</section>

		<!-- Some Database statistics: -->

		<section id="databases">
			<div class="container">

				<div class="section-title text-center center">
					<h2>Affect Databases</h2>
					<hr>
				</div>

				<div class="row">
					 <div class="col-md-6">
						<h4>A New Challenge...</h4>
						<p>In recent years, the research area of emotion detection and analysis has faced the need of analysing images showing natural behaviours, rather than controlled ones. Thus, a new challenge that appeared was improving previous databases (like ’MMI database’ or ‘Multi-Pie database’) which consist of images (or frames) displaying posed behaviour only(acted emotions on comands). The new approach was trying to capture subjects in an “in the wild" environment,                          so that the accuracy of the analysis could increase and be more representative for real life situations. However, even such existing datasets have proved to be limited, in the sense that they did not provide enough consistency in order to properly analyse the different representations of emotions. We present here 3 databses which reflect these limitations, having made a comparison table: RECOLA, AFEW and AFEW-VA, and then introduce the new dataset Aff-Wild, tailored to the needs of capturing more genuine expressions.</p>
					</div>

					<div class="col-md-6">
						<h4>Overcoming Limitations</h4>
						<p>The limitations of existing databases like RECOLA, AFEW or AFEW-VA were mainly caused by controlled laboratory conditions (solely in case of RECOLA), reduced number of captured frames and very few annotators. A 'controlled laboratory condition' is a situation in which the subject does not show certain emotions in a natural mannner, but is requested to more likely 'imitate' a certain feeling (e.g. happiness). This simulated emotion is then 'annotated' by a specialised person,called an 'annotator', who classifies given images and videos by different criteria on the basis of the '7 basic emotions' categories. Moreover, they can also assign values of valence and arousal, the annotated information being then used for training. In case of the RECOLA dataset, the subjects were recorded for a total of 9.5 hours and then the annotation process was carried out by 6 people. The main downside of this (apart from the controlled environment) is the very small number of videos. In contrast,
                        the AFEW dataset, which consists of real-world situations, has over 1800 analysed videos, but its drawbacks are that the annotation is restricted to the basic seven expressions and that the total number of frames, which is approximately 3 times smaller than the one of RECOLA. Finally, AFEW-VA is limited by the very reduced number of frames (more than 10 times smaller than RACOLA’s) and the discrete values of valance and arousal.</p>
					</div>

				</div>

				<div class="section-title text-center center">
					<h2>Comparison</h2>
					<hr>
				</div>

				<div class="row text-center">
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-database"> </i>
							<h4>DATABASES:</h4>
							<h3>RECOLA</h3>
							<h3>AFEW</h3>
							<h3>AFEW-VA</h3>
							<h3>Aff-Wild</h3>
						</div>

					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-film"></i>
							<h4>NO. OF FRAMES:</h4>
							<div class="counter">
								<div class="counter-value" data-count="345000">0</div>
								<div class="counter-value" data-count="113355">0</div>
								<div class="counter-value" data-count="30050">0</div>
								<div class="counter-value" data-count="1180000">0</div>
							</div>
						</div>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-video"> </i>
							<h4>NO. OF VIDEOS:</h4>
							<div class="counter">
								<div class="counter-value" data-count="46">0</div>
								<div class="counter-value" data-count="1809">0</div>
								<div class="counter-value" data-count="600">0</div>
								<div class="counter-value" data-count="298">0</div>
							</div>
						</div>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="data">
							<i class="fas fa-file-signature"> </i>
							<h4>ANNOTATORS:</h4>
							<div class="counter">
								<div class="counter-value" data-count="6">0</div>
								<div class="counter-value" data-count="4">0</div>
								<div class="counter-value" data-count="2">0</div>
								<div class="counter-value" data-count="8">0</div>
							</div>
						</div>
					</div>
				</div>

				<br>

				<div class="row">
					<div class="text">
						<h4>The Aff-Wild</h4>
						<p>The existing limitations is the reason why Dimitrios Kollias and his colleagues started the development of the Aff-Wild, a dataset which consists of more than 30 hours of recordings from 298 videos. The main goal of the new dataset was to gather frames reflecting spontaneous behaviour of the subjects. They were captured from a series of YouTube videos, in which people react to a variety of situations,
						for instance an unexpected scenario in a film, or eating something that they have never eaten before. An important feature of the Aff-Wild is represented by the different cultural backgrounds of the subjects as well as the diverse illumination and occlusion conditions. Another improvement brought by the Aff-Wild is the [-1, 1] interval, in which the valence and arousal can range in a continuous manner, the annotation process being carried out by 8 annotators. As an overview, the Aff-Wild database provides a very powerful methodology for analysing not only static emotional states of the subjects, but also fast transitions of the facial expressions. </p>

						<p>Aff-Wild surpasses previous databases, as some parts of it have been also annotated for the seven basic expressions and action units. This makes the Aff-Wild the first really big dataset, in-the-wild that is also anotated for all the three described affect categories (seven basic emotions, action units, valence-arousal space).</p>
						<p>"When it comes to dimensional emotion recognition, there exists great variability between different databases, especially those containing emotions in-the-wild. In particular, the annotators and the range of the annotations are different and the labels can be either discrete or continuous. To tackle the problems caused by this variability, we take advantage of the fact that the Aff-Wild is a powerful database
						that can be exploited for learning features, which may then be used as priors for dimensional emotion recognition" (information taken from the paper of the database, linked below).</p>
						<p>Want a more in depth read about this dataset? You can read Dimitrios and his colleagues' paper on this: <a href="https://arxiv.org/pdf/1804.10938.pdf" target=_blank>Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond</a></p>

					</div>
				</div>

				<div class="row">
					<div class="text">
						<h4>But How Was Aff-Wild Used?</h4>
						<p>A Challenge was organised to see diferent image recognition algorithms. The training set of the Aff-Wild database has 1,008,650 frames (that were used for training networks) while the test set consists of 215,450 frames. In total, ten different research groups downloaded the Aff-Wild database. Six of them made experiments and submitted their results to the workshop portal. Based on the performance they obtained on the test data, three of them were selected to present their results to the workshop. Comparated to a lab conditions database, the results were lower.</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<h4>Creating An Even Better Architecture</h4>
						<p>The results were promising, but there was room left for improvement. This is why Dimitrios and his colleagues' also created a database, AffWildNet that underwent the same challenge. More details about this are described in <a class="page-scroll" href="#howitworks">How It Works Section.</a> The AffWildNet is a CNN-RNN network. This architecture managed to outperform the other architectures developed in the challenge, because of the network design and the network training.
						The architecture was then fine tuned to benefit from the learning features of Aff-Wild.</p>

						<p>You can see in the tables on the right that AffWildNet clearly outperforms its' counterparts. The criteria (that was demanded in the Aff-Wild Challenge) was the CCC (the Concordance Correlation Coefficient), which is widely used for measuring performance of emotion recognition architectures. CCC penalises proportionally any discrepancies between the annotated values and the prediction made by the architectures (note, this is a simplified explanation of the whole process).</p>
					</div>

					<div class="col-md-6 text-center">
						<img src="img/challengetable.png" class="img-responsive text-center" alt="Aff-Wild Challenge Results">
						<p>The results obtained by the finalist teams in the Aff-Wild Challenge</p>
						<img src="img/affwildnet.png" class="img-responsive text-center" alt="AffWildNet Performance Against Pre-Trained Networks">
						<p>AffWildNet Performance Against Pre-Trained Networks and VGG-Face-LSTM architecture</p>
						<p>Source: <a href="https://arxiv.org/pdf/1804.10938.pdf" target=_blank>Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond</a></p>
					</div>
				</div>

				<div class="section-title text-center center">
					<h2>Multi-tasking</h2>
					<hr>
				</div>

				<div class="row">
					<div class="text">
						<p>A strong question that relates to databases is how can we best combine different criterias. How can we create an architecture that is able to recognise the seven basic emotion but also be able to scale frames on the valence-arousal space? The same question applies for image generation. The appearance of Aff-Wild facilitated the exploration of those questions, as it incorporates the three presented criterias.</p>
					</div>
				</div>

				<div class="row">


					<div class="col-md-6">
						<h4>Creating A Multi-Task Architecture</h4>
						<p>This machine learning paradigm (The Multi-Task Learning, or simply MTL) provides an opportunity to find similarities between a number of different tasks and significantly improve the performance of a single task. This is possible due to the ability to include other tasks (which are linked to the current one) in the process of training the data. Dimitrios Kollias and his colleague contributed to solving the multi-task problem by the following means: first, by annotating a fraction of the Aff-Wild dataset in terms of the seven fundamental emotional states, and another fraction in terms of 8 action units.
						This provided a better overview on the fast changes between two emotion representations, highlighting the broad palette of emotions as well as the variability of behaviours. The second contribution was done through modifying and improving the existing so called ‘GAN' architecture (Generative Adversarial Network), so that it was possible to obtain realistic images of all subjects who show up in the Aff-Wild, as well as representations of new emotional states of the persons already in the database.</p>

						<p>More information about this can be found on the <a href="https://arxiv.org/pdf/1811.07771.pdf" target=_blank>A Multi-Task Learning & Generation Framework: Valence-Arousal, Action Units & Primary Expressions</a></p>
					</div>

					<div class="col-md-6 text-center">
						<br><br><br><br><br>
						<i class="fas fa-tasks" style="font-size: 250px;"> </i>
					</div>
				</div>

			</div>
		</section>

		<!-- How it works Section: -->

		<section id="howitworks">
			<div class="container">
				<div class="section-title text-center center">
					<h2>How It Works</h2>
					<hr>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<br><br><br>
						<i class="fas fa-desktop"></i>
					</div>
					<div class="col-md-6">
						<h4>"Older" Methods - Computer Vision</h4>
						<p>Computer Vision proved in time to be a multi purpose tool for a variety of uses. One of those uses today is recognising and generating affects.</p>
						<p>Different parts of the face would be evaluated, and depending on the patterns of muscle contractions, they would be categorised in one of the 7 basic emotions (both eyebrows lifted and mouth wide opened most likely means that the person is scared or surprised). This was evaluated by measuring different body parts and what percentage of the picture they occupy, or the distance between the main components of the face (a higher distance between eyes and eyebrows means surprise).</p>
					</div>
				</div>

				<br><br>

				<div class="row">
					<div class="col-md-6">
						<h4>"Younger" Methods - Neural Networks</h4>
						<p>The recent popularity of neural networks also expanded in this field, due to their high versatility. Through training, they become able to recognise a various range of emotions, also becoming able to mix different affects. Two types of neural networks are used, namely CNNs (Convolutional Neural Networks) and RNNs (Recurrent Neural Networks).</p>
					</div>
					<div class="col-md-6 text-center">
						<br><br><br>
						<i class="fas fa-network-wired"></i>
					</div>
				</div>

				<br><br>

				<div class="section-title text-center center">
					<h2>CNNs and RNNs</h2>
					<h2>Image Recognition</h2>
					<hr>
				</div>

				<div class="row">
					<div class="col-md-6 text-center">
						<br><br>
						<img src="img/cnn.png" class="img-responsive" alt="A CNN Example">
						<p>A Basic CNN Example</p>
						<p>Source: Wikipedia, <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target=_blank>Convolutional Neural Networks</a></p>
					</div>
					<div class="col-md-6">
						<h4>CNNs - Convolutional Neural Networks</h4>
						<p>Convolution neural network (CNNs) is a class of deep neural networks comprised of two main stages: feature extraction and image classification. CNNs are commonly used in the domain of image processing and can be applied to the specific task of emotional recognition.
						   CNNs utilise supervised learning techniques, therefore, a labeled data set is required to train them to perform a specific task. For the task of identifying emotions, a CNN could be trained using a dataset containing many different images of human faces, which have been classified into one of 7 states or valence arousal by psychologists. The accuracy and reliability of the trained neural network is heavily dependent upon the quality of the database used in training. </p>
					</div>
				</div>

				<div class="row">
					<div class="text">
						<p>Essentially, as the name implies, CNNs use convolutions to fulfill their tasks. Those can be understood as “stages” of processing an image. We first start from a 2D image that our brains can easily understand. Then the neural networks starts transposing different parts of the image in higher dimensions (that oftentimes reach great values, even as far as 50D) that are known as hyperspace. This is called spatial analysis, as certain futures are evaluated at different “spaces” (dimensions).
						Each convolution, depending on the results from the previous one, updates itself. After reaching a certain number of convolutions, the networks simply gives the classification it made. A result from a convolution is obtained by attaching a different weight to different features. The weight is then updated by further convolutions (high values means that analysed feature is pronounced and is part of the image). Furthermore, the black box property means that we can’t specify or completely comprehend the “features” that the CNN will pick out.
						As it is trained, it will learn its own patterns and signals that (as a machine) it would notice which may not necessarily be the same markers that we as humans use for emotion recognition.</p>
					</div>
				</div>

				<div class="row">
					<div class="col-md-6">
						<h4>RNNs - Recurrent Neural Networks</h4>
						<p>Recurrent Neural Networks take in an input state as well as an input to process, functioning similar to a state machine. Their main feature is that it can use the same node to process all of the data. However, even given the same input the node or ‘cell’ could produce a different output. This is because the output also depends on the previous state that is fed into the cell. As a result RNNs are effective at pattern replication and sequence prediction: given an input, they can produce an output, depending on previous patterns and sequences.
						   This is useful when dealing with data that comes in a specified sequence, such as text or video. The cells themselves apply some mathematical functions to the input, dependent on the input state, and then produce both an output of the network and another state, that can be fed back into itself for the next input.
						   Alternatively, the cells can be “rolled out” so that there are several cells in sequence, so that certain cells could apply functions that are slightly different, or have differently adjusted hyperparameters in their functions.
						   For example, if a sentence was passed into a sequence of RNN cells, you might expect that some of the middle cells would be more likely to output verbs, and the opposite for the last ones, as it's unlikely to end a sentence in a verb.</p>
					</div>
					<div class="col-md-6 text-center">
						<br><br><br>
						<img src="img/rnns.png" class="img-responsive" alt="A RNN Diagram">
						<p>This shows what the example of a basic RNN may look like. Here the network is 2 cells deep (hence it could be referred to a deep RNN), and is rolled out over a sequence of 6 cells.</p>
						<p>Source: <a href="https://www.youtube.com/watch?v=u4alGiomYP4" target=_blank>Martin Gorner, TensorFlow and Deep Learning without a PhD, Lecture @ Google Cloud Next ’17</a></p>
					</div>
				</div>

				<div class="row">
					<div class="text">
						<p>Furthemore, these cells can be stacked on top of each other, allowing for a second round of data processing(that may be looking at a different aspect of the data) to be done. This would also carry its own input state so that there would be several input/output states used.
						This approach somewhat mimics the parallel processing approach used in CNNs when they utilise several layers to detect a different part of input data.</p>
						<p>Within the realm of emotion recognition, both CNNs and RNNs are used for different purposes, but together to solve the ultimate goal of recognising and then assigning a specific value to facial expressions.
						We can also process not only images but whole videos. This is where the RNNs are used. The RNNs are specifically made for and used for handling temporal data, or when there needs to be some sort of short-term memory to process data and interpret it by also remembering what came before it.
						In a video (essentially a sequence of images) we can pick out some of the key frames of the video and . We don’t utilise every frame as there is a miniscule difference between every pair of neighbouring frames and processing all of them would result in wasted computational power and time. Then when the RNN calculates its output of what emotion it believes is being represented by the video, it utilises the sequence of frames before hand to see how the face was changing.
						For example, if the first processed frame was a neutral face, then the eyes started the widen, then in the next frame the mouth began to open, then the cheeks began to raise and tense, this sequence of actions could indicate that the person is happily surprised or excited. However, from those individual frames it may be much harder to realise that is the emotion that is being expressed. So in combination, the CNN works on the feature extraction of each frame, and the RNN interprets how that sequence of frames is connected to recognise an emotion.</p>
					</div>
				</div>

				<div class="section-title text-center center">
					<h2>Image Generation</h2>
					<hr>
				</div>

				<div class="row">
					<div class="text">
						<p>While being able to recognise, classify and quantify emotions have some obvious applications and uses, the research field surrounding the generation of synthetic emotions onto faces is equally intriguing. It has applications in facial recognition and verification, emotion prediction, and for use in the entertainment industry.</p>
						<p>When the generative model is training, one essential requirement is a large and varied dataset. There must be a significant number of different people in the data set, with as large a range of facial structures and features as possible. Therefore, when the model is trained, it learns how to map emotions onto new faces, as opposed to trying to morph the input face into the faces of the participants in the sample data. In his work on a generative model, they used the 4DFAB database a large 4-dimensional facial database. The data is 4 dimensional as it is comprised of 3-D images of faces showing different emotions, being joined together to form videos of people transitioning between emotions. To the best of his knowledge, the approach of using multi-dimensional data when synthesising face images has not been done before.</p>
						<p>The extra physical dimension helps with the problems of pose variation and occlusion when trying to recognise and accurately track “in-the-wild” facial features. The variety also helps the model to differentiate between what patterns are representative of a person’s facial features, and what actually corresponds to a specific expressed emotion. Furthermore, by having video to train models on, the generative network can learn how to more accurately change an input image’s neutral expression into a specified emotion.</p>
						<p>This has mainly been achieved through the development of Generative Adversarial Networks (GANs) by Ian Goodfellow et al in 2014. GANs have allowed for image analysis and transformation move away from a pixel-by-pixel approach, to a more feature and pattern-based system. Not only is this more general approach more effective when looking at unseen data and ‘subjects’, but it reveals a more semantic meaning behind what the network is doing to the image.</p>
						<p>In short, Generative Adversarial Networks are two different models working against each other to improve each other. Here, one network is generating images with generated facial affects, which is then fed into a second network which is classifying emotions and trying to detect if the images are real. So the first network is trying to fool the second, and both learn and improve on how best to do that. This is the main concept behind GANs.</p>
					</div>
				</div>

			</div>
		</section>

		<!-- About Section -->

		<section id="about">
			<div class="container">

				<div class="section-title text-center center">
					<h2>About</h2>
					<hr>
				</div>

				<div class="row">
					<div class="col-md-3 text-center">
						<img src="img/dimitrios.jpg" class="img-circle" alt="Dimitrios Kollias" width="180" height="230">
							
						<h3>Dimitrios Kollias</h3>
						<p>Assistant Researcher, PhD Student
						<br>Department of Computing
						<br>Imperial College London, UK
						<br> <i class="fab fa-researchgate"> </i> <a href="https://www.researchgate.net/profile/Dimitrios_Kollias2" target=_blank style="color: #0cb;">   Research Gate</a>
						<br><a>dimitrios.kollias15@imperial.ac.uk</a></p>
					</div>
				</div>

		   <div class="row">
				 <div class="col-md-15 text-center">
				 <h2> Publications </h2>
				 <br>
				 <hr>
			 </div>
		 </div>

		 <div class="row">
		 </div>
    </div>
		</section>

		<section id="reference">
			<div class="container">
			</div>
		</section>

		<!-- The footer -->

		<section id="footer">
			<div class="container">
				<div class="row">
					<div class="col-md-3 col-sm-6">
						<div class="row">

							<div class="cod-md-5 col-sm-7">
								<i class="far fa-user" data-toggle="modal" data-target="#modalCalin"> </i>
								<p>Calin</p>
							</div>

							<div class="cod-md-4 col-sm-3">
								<i class="far fa-user" data-toggle="modal" data-target="#modalAndrei"> </i>
								<p>Andrei</p>
							</div>
						</div>
					</div>
					<div class="col-md-6 col-sm-6">
						<h3>Designed by Andrei Roman, Calin Biberea, Prabhjot Grewal, Daniel Kaye</h3>
						<p>Click the icons to see some information about them.</p>
					</div>
					<div class="col-md-3 col-sm-6">
						<div class="row">
							<div class="cod-md-5 col-sm-7">
								<i class="far fa-user" data-toggle="modal" data-target="#modalPrabhjot"> </i>
								<p>Prabhjot</p>
							</div>
							<div class="cod-md-4 col-sm-3">
								<i class="far fa-user" data-toggle="modal" data-target="#modalDaniel"> </i>
								<p>Daniel</p>
							</div>
						</div>
					</div>
				</div>
			</div>

			<div id="modalCalin" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Calin Biberea</h4>
						</div>
						<div class="modal-body">
							<div class="row">
								<div class="col-md-3">
									<img src="img/calin.jpg" class ="img-responsive" alt="A Picture of Calin">
								</div>
								<div class="col-md-9">
									<h4>Undergraduate Student - Department of Computing</h4>
									<h4 style="color:blue;">Imperial College London</h4>
									<h4>Course: MEng Computing (Software Engineering)</h4>
								</div>
							</div>
						</div>
						<div class="modal-footer">
							<p>Want to contact Calin? Mail him at <a>cb3418@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalAndrei" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Andrei Roman</h4>
						</div>
						<div class="modal-body">
							<div class="row">
								<div class="col-md-3">
									<img src="img/andrei.jpg" class ="img-responsive" alt="A Picture of Andrei">
								</div>
								<div class="col-md-9">
									<h4>Undergraduate Student - Department of Computing</h4>
									<h4 style="color:blue;">Imperial College London</h4>
									<h4>Course: BEng Computing</h4>
								</div>
							</div>
						</div>
						<div class="modal-footer">
							<p>Want to contact Andrei? Mail him at <a>amr818@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalPrabhjot" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Prabhjot Grewal</h4>
						</div>
						<div class="modal-body">
							<div class="row">
								<div class="col-md-3">
									<img src="img/prab.jpg" class ="img-responsive" alt="A Picture of Prabhjot">
								</div>
								<div class="col-md-9">
									<h4>Undergraduate Student - Department of Computing</h4>
									<h4 style="color:blue;">Imperial College London</h4>
									<h4>Course: BEng Computing</h4>
								</div>
							</div>
						</div>
						<div class="modal-footer">
							<p>Want to contact Prabhjot? Mail him at <a>psg918@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>

			<div id="modalDaniel" class="modal fade" role="dialog">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-header">
							<button type="button" class="close" data-dismiss="modal">&times;</button>
							<h4 class="modal-title">Daniel Kaye</h4>
						</div>
						<div class="modal-body">
							<div class="row">
								<div class="col-md-3">
									<img src="img/daniel.jpg" class="img-responsive" alt="A Picture of Daniel">
								</div>
								<div class="col-md-9">
									<h4>Undergraduate Student - Department of Computing</h4>
									<h4 style="color:blue;">Imperial College London</h4>
									<h4>Course: MEng Computing (Artificial Intelligence)</h4>
								</div>
							</div>
						</div>
						<div class="modal-footer">
							<p>Want to contact Daniel? Mail him at <a>djk18@ic.ac.uk</a></p>
						</div>
					</div>
				</div>
			</div>


		</section>

		<!-- JQuery import: -->

		<script src="js/JQuery.js"></script>

		<!-- Other javascripts that are imported: -->

		<script src="js/bootstrap.js"></script>
		<script src="js/SmoothScroll.js"></script>
		<script src="js/jquery.isotope.js"></script>
		<script src="js/jquery.parallax.js"></script>
		<script src="js/particles/particles.js"></script>
		<script src="js/particles/app.js"></script>

		<!-- Javascripts that we created: -->

		<script src="js/main.js"> </script>
		<script src="js/header.js"> </script>
		<script src="js/gallery.js"> </script>
		<script src="js/counter.js"> </script>

	</body>

</html>
